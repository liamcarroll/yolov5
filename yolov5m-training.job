#!/bin/bash
#SBATCH -p compute    # which partition to run on ('compute' is default)
#SBATCH -J yolov5m-training    # arbitrary name for the job (you choose)
#SBATCH --gres=gpu:tesla:{2}
#SBATCH --cpus-per-task=4    # tell Slurm how many CPU cores you need, if different from default; your job won't be able to use more than this
#SBATCH --mem=0    # how much RAM you need (30GB in this case), if different from default; your job won't be able to use more than this
#SBATCH --mail-type={ALL} --mail-user={liam.carroll36@mail.dcu.ie}

# Uncomment the following to get a log of memory usage; NOTE don't use this if you plan to run multiple processes in your job and you are placing "wait" at the end of the job file, else Slurm won't be able to tell when your job is completed!
# vmstat -S M {interval_secs} >> memory_usage_$SLURM_JOBID.log &

# Your commands here
%load_ext tensorboard
%tensorboard --logdir runs/train

import wandb
wandb.login()

python train.py --img 960 --batch 64 --epochs 5 --data uadetrac-training.yaml --weights yolov5m.pt